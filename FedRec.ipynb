{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPavSJRbXdSe6geP9hC3XyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishnoiAman777/Efficient-FedRec/blob/main/FedRec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Structure"
      ],
      "metadata": {
        "id": "1Txx0qGmrWtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "abwRkW0Grdet"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9kG6uMq2rZq8"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MIND Dataset"
      ],
      "metadata": {
        "id": "xsxiiYWySc1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Dataset"
      ],
      "metadata": {
        "id": "4BnqQRqkSfQj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir Dataset/train\n",
        "!mkdir Dataset/valid\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip -P Dataset/train\n",
        "!unzip Dataset/train/MINDsmall_train.zip -d Dataset/train/\n",
        "!rm Dataset/train/MINDsmall_train.zip\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDsmall_dev.zip -P Dataset/valid\n",
        "!unzip Dataset/valid/MINDsmall_dev.zip -d Dataset/valid/\n",
        "!rm Dataset/valid/MINDsmall_dev.zip\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_test.zip -P Dataset/test\n",
        "!unzip Dataset/test/MINDlarge_test.zip -d Dataset/test/\n",
        "!rm Dataset/test/MINDlarge_test.zip"
      ],
      "metadata": {
        "id": "5f49i2c5Skim"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing for a user"
      ],
      "metadata": {
        "id": "b63nUBnX4b3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "behaviour_df = pd.read_csv('/content/Dataset/train/behaviors.tsv',sep = '\\t', index_col=0, names=['imp_id', 'UserId', 'Time', 'History', 'Samples'])"
      ],
      "metadata": {
        "id": "8TZQBCVj4gKj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out the unique users in the dataframe\n",
        "users = behaviour_df.UserId\n",
        "print(\"Number of unique users are\", users.nunique())\n",
        "# Let's take out a user at random and try to make a client model based on that only\n",
        "luckyUser = random.choice(list(users.unique()))\n",
        "print(\"The lucky user is for our experiment is\", luckyUser)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7DW6UfB5xXU",
        "outputId": "b0a59b7c-4d86-4285-eddc-4f5841750e84"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique users are 50000\n",
            "The lucky user is for our experiment is U10297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def saveLuckyUserData(behaviour_csv_path, news_csv_path, UID, folder_type):\n",
        "    behaviour_df = pd.read_csv(behaviour_csv_path, sep = '\\t', index_col=0, names=['imp_id', 'UserId', 'Time', 'History', 'Samples'])\n",
        "    luckyUser_behavoiur = behaviour_df[behaviour_df.UserId == UID]\n",
        "    news_by_user = []\n",
        "    for index, row in luckyUser_behavoiur.iterrows():\n",
        "        his = row.History\n",
        "        news_by_user.extend(his.split())\n",
        "        sam = [j[0] for j in [i.split(\"-\") for i in row.Samples.split()]]\n",
        "        news_by_user.extend(sam)\n",
        "    news_df = pd.read_csv(news_csv_path,sep = '\\t', names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Title Entities', 'Abstract Entites'])\n",
        "    lucky_news_df = news_df.query('NewsID.isin(@news_by_user)')\n",
        "    lucky_news_df.to_csv(f\"LuckyUserDataset/{folder_type}/news.csv\", header=False)\n",
        "    luckyUser_behavoiur.to_csv(f\"LuckyUserDataset/{folder_type}/behaviour.csv\", header=False)"
      ],
      "metadata": {
        "id": "aiol6FML4cAK"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir LuckyUserDataset\n",
        "!mkdir LuckyUserDataset/train\n",
        "!mkdir LuckyUserDataset/test\n",
        "!mkdir LuckyUserDataset/valid"
      ],
      "metadata": {
        "id": "_yXWSUlb6mZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad33cdd4-c01b-4987-cf5d-1c45f3039011"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘LuckyUserDataset’: File exists\n",
            "mkdir: cannot create directory ‘LuckyUserDataset/train’: File exists\n",
            "mkdir: cannot create directory ‘LuckyUserDataset/test’: File exists\n",
            "mkdir: cannot create directory ‘LuckyUserDataset/valid’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saveLuckyUserData(\"/content/Dataset/train/behaviors.tsv\", \"/content/Dataset/train/news.tsv\", luckyUser, \"train\")\n",
        "saveLuckyUserData(\"/content/Dataset/test/behaviors.tsv\", \"/content/Dataset/test/news.tsv\", luckyUser, \"test\")\n",
        "saveLuckyUserData(\"/content/Dataset/valid/behaviors.tsv\", \"/content/Dataset/valid/news.tsv\", luckyUser, \"valid\")"
      ],
      "metadata": {
        "id": "dAT5QwX-6x4d"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Lucky User Data"
      ],
      "metadata": {
        "id": "UXohyFC6vbLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post_behaviour_df_train = pd.read_csv(f\"/content/LuckyUserDataset/train/behaviour.csv\", index_col=0, names=['imp_id', 'UserId', 'Time', 'History', 'Samples'])\n",
        "post_behaviour_df_test = pd.read_csv(f\"/content/LuckyUserDataset/test/behaviour.csv\", index_col=0, names=['imp_id', 'UserId', 'Time', 'History', 'Samples'])\n",
        "post_behaviour_df_valid = pd.read_csv(f\"/content/LuckyUserDataset/valid/behaviour.csv\", index_col=0, names=['imp_id', 'UserId', 'Time', 'History', 'Samples'])"
      ],
      "metadata": {
        "id": "5vm-WpasxANh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_imprs = defaultdict(list)\n",
        "\n",
        "# read user impressions\n",
        "for index, l in tqdm(post_behaviour_df_train.iterrows()):\n",
        "    imp_id, uid, t, his, imprs = index, l.UserId, l.Time, l.History, l.Samples\n",
        "    his = his.split()\n",
        "    imprs = [i.split(\"-\") for i in imprs.split(\" \")]\n",
        "    neg_imp = [i[0] for i in imprs if i[1] == \"0\"]\n",
        "    pos_imp = [i[0] for i in imprs if i[1] == \"1\"]\n",
        "    user_imprs[uid].append([imp_id, his, pos_imp, neg_imp,0, uid])\n",
        "\n",
        "for index, l in tqdm(post_behaviour_df_valid.iterrows()):\n",
        "    imp_id, uid, t, his, imprs = index, l.UserId, l.Time, l.History, l.Samples\n",
        "    his = his.split()\n",
        "    imprs = [i.split(\"-\") for i in imprs.split(\" \")]\n",
        "    neg_imp = [i[0] for i in imprs if i[1] == \"0\"]\n",
        "    pos_imp = [i[0] for i in imprs if i[1] == \"1\"]\n",
        "    user_imprs[uid].append([imp_id, his, pos_imp, neg_imp, 1, uid])\n",
        "\n",
        "for index, l in tqdm(post_behaviour_df_test.iterrows()):\n",
        "    imp_id, uid, t, his, imprs = index, l.UserId, l.Time, l.History, l.Samples\n",
        "    his = his.split()\n",
        "    imprs = imprs.split(\" \")\n",
        "    user_imprs[uid].append([imp_id, his, imprs, [], 2, uid])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4amNxOicvhxx",
        "outputId": "daf71b18-c742-4fc3-f9db-54bec09ee22e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2it [00:00, 1175.70it/s]\n",
            "0it [00:00, ?it/s]\n",
            "3it [00:00, 2740.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_his_len=50\n",
        "npratio = 4\n",
        "out_path = \"LuckyUserDataset\"\n",
        "train_samples = []\n",
        "valid_samples = []\n",
        "test_samples = []\n",
        "user_indices = defaultdict(list)"
      ],
      "metadata": {
        "id": "hvMwAD26z71b"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the `Behaviour` LuckyUserData"
      ],
      "metadata": {
        "id": "M5ZsPPQi082b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "for uid in tqdm(user_imprs):\n",
        "    for impr in user_imprs[uid]:\n",
        "        imp_id, his, poss, negs, is_valid, uid = impr\n",
        "        his = his[-max_his_len:]\n",
        "        if is_valid == 0:\n",
        "            for pos in poss:\n",
        "                train_samples.append([imp_id, pos, negs, his, uid])\n",
        "                user_indices[uid].append(index)\n",
        "                index += 1\n",
        "        elif is_valid == 1:\n",
        "            valid_samples.append([imp_id, poss, negs, his, uid])\n",
        "        else:\n",
        "            test_samples.append([imp_id, poss, negs, his, uid])\n",
        "\n",
        "print(len(train_samples), len(valid_samples), len(test_samples))\n",
        "\n",
        "with open(out_path + \"/train_sam_uid.pkl\", \"wb\") as f:\n",
        "    pickle.dump(train_samples, f)\n",
        "\n",
        "with open(out_path + \"/valid_sam_uid.pkl\", \"wb\") as f:\n",
        "    pickle.dump(valid_samples, f)\n",
        "\n",
        "with open(out_path + \"/test_sam_uid.pkl\", \"wb\") as f:\n",
        "    pickle.dump(test_samples, f)\n",
        "\n",
        "with open(out_path + \"/user_indices.pkl\", \"wb\") as f:\n",
        "    pickle.dump(user_indices, f)\n",
        "\n",
        "train_user_samples = 0\n",
        "\n",
        "for uid in tqdm(user_indices):\n",
        "    train_user_samples += len(user_indices[uid])\n",
        "\n",
        "print(train_user_samples / len(user_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPKfEeY3vhqg",
        "outputId": "bd372fec-f4cd-4e7a-8ef8-5f8587c053a2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 6574.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 0 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 562.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the `News` LuckyUserData"
      ],
      "metadata": {
        "id": "eNNyTEBI1DEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "F2UyCivrvhjq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_title_len= 30\n",
        "post_news_df_train = pd.read_csv('/content/LuckyUserDataset/train/news.csv', names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Title Entities', 'Abstract Entites'])\n",
        "post_news_df_test = pd.read_csv('/content/LuckyUserDataset/test/news.csv', names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Title Entities', 'Abstract Entites'])\n",
        "post_news_df_valid = pd.read_csv('/content/LuckyUserDataset/valid/news.csv', names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Title Entities', 'Abstract Entites'])"
      ],
      "metadata": {
        "id": "xjPcYda01Wjy"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# news preprocess\n",
        "nid2index = {\"<unk>\": 0}\n",
        "news_index = [[[0] * max_title_len, [0] * max_title_len]]\n",
        "for index, l in post_news_df_train.iterrows():\n",
        "    nid, vert, subvert, title, abst, url, ten, aen = l.NewsID, l.Category, l.SubCategory, l.Title, l.Abstract, l.URL, l[\"Title Entities\"], l[\"Abstract Entites\"]\n",
        "    if nid in nid2index:\n",
        "        continue\n",
        "    tokens = tokenizer(\n",
        "        title,\n",
        "        max_length=max_title_len,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    nid2index[nid] = len(nid2index)\n",
        "    news_index.append([tokens.input_ids, tokens.attention_mask])\n",
        "\n",
        "for index, l in post_news_df_valid.iterrows():\n",
        "    nid, vert, subvert, title, abst, url, ten, aen = l.NewsID, l.Category, l.SubCategory, l.Title, l.Abstract, l.URL, l[\"Title Entities\"], l[\"Abstract Entites\"]\n",
        "    if nid in nid2index:\n",
        "        continue\n",
        "    tokens = tokenizer(\n",
        "        title,\n",
        "        max_length=max_title_len,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    nid2index[nid] = len(nid2index)\n",
        "    news_index.append([tokens.input_ids, tokens.attention_mask])\n",
        "\n",
        "with open(out_path + \"/bert_nid2index.pkl\", \"wb\") as f:\n",
        "    pickle.dump(nid2index, f)\n",
        "\n",
        "news_index = np.array(news_index)\n",
        "np.save(out_path + \"/bert_news_index\", news_index)\n",
        "\n",
        "# Preparing the test data\n",
        "\n",
        "nid2index = {\"<unk>\": 0}\n",
        "news_index = [[[0] * max_title_len, [0] * max_title_len]]\n",
        "\n",
        "for index, l in post_news_df_test.iterrows():\n",
        "    nid, vert, subvert, title, abst, url, ten, aen = l.NewsID, l.Category, l.SubCategory, l.Title, l.Abstract, l.URL, l[\"Title Entities\"], l[\"Abstract Entites\"]\n",
        "    if nid in nid2index:\n",
        "        continue\n",
        "    tokens = tokenizer(\n",
        "        title,\n",
        "        max_length=max_title_len,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    nid2index[nid] = len(nid2index)\n",
        "    news_index.append([tokens.input_ids, tokens.attention_mask])\n",
        "\n",
        "with open(out_path + \"/bert_test_nid2index.pkl\", \"wb\") as f:\n",
        "    pickle.dump(nid2index, f)\n",
        "\n",
        "news_index = np.array(news_index)\n",
        "np.save(out_path + \"/bert_test_news_index\", news_index)\n"
      ],
      "metadata": {
        "id": "brfpt3hs1ROD"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training User model"
      ],
      "metadata": {
        "id": "sKcO-E5OSv0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this Experiment I will be using DistillBert for generating Embeddings compared to Bert the original paper used because it has 40% less parameters and can run smoothly on client side"
      ],
      "metadata": {
        "id": "Op53E20B6YuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "data_path = Path(\"LuckyUserDataset\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "yC9yPQYeToiO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### News Dataset Class:\n",
        "This class just returns distil_bert title and it's curresponding mask for a particular news"
      ],
      "metadata": {
        "id": "Pn8BSNWSVnZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This class just returns distil_bert title and it's curresponding mask for a particular news\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, news_index):\n",
        "        self.news_index = news_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.news_index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.news_index[idx]"
      ],
      "metadata": {
        "id": "WAr-0HM2UdeU"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### News Part Dataset\n",
        "Return `<token, attention_mask>` curresponding to a particular NewsID"
      ],
      "metadata": {
        "id": "OZkG-G9FjBPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsPartDataset(Dataset):\n",
        "    def __init__(self, news_index, nids):\n",
        "        self.news_index = news_index\n",
        "        self.nids = nids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.nids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        nid = self.nids[idx]\n",
        "        return nid, self.news_index[nid]"
      ],
      "metadata": {
        "id": "gnpsi8wIjA4g"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additive Attention\n",
        "This class contains the additive Attention Module that contains word Embeddings into a sentence Embeddings"
      ],
      "metadata": {
        "id": "n8IoLRwCcc-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, d_h, hidden_size=200):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "        self.att_fc1 = nn.Linear(d_h, hidden_size)\n",
        "        self.att_fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        bz = x.shape[0]\n",
        "        e = self.att_fc1(x)\n",
        "        e = nn.Tanh()(e)\n",
        "        alpha = self.att_fc2(e)\n",
        "        alpha = torch.exp(alpha)\n",
        "        if attn_mask is not None:\n",
        "            alpha = alpha * attn_mask.unsqueeze(2)\n",
        "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "        x = torch.bmm(x.permute(0, 2, 1), alpha)\n",
        "        x = torch.reshape(x, (bz, -1))  # (bz, 400)\n",
        "        return x"
      ],
      "metadata": {
        "id": "P3uMGlGFcck2"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Encoder\n",
        "This class contains the text Encoder DistillBert + Additive Attention which will return the textual Embedding of a particular News"
      ],
      "metadata": {
        "id": "VjHU42lccLA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 word_embedding_dim=400,\n",
        "                 dropout_rate=0.2,\n",
        "                 enable_gpu=True):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.dropout_rate = 0.2\n",
        "        self.DistillBert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.additive_attention = AdditiveAttention(self.DistillBert.config.hidden_size,\n",
        "                                                    self.DistillBert.config.hidden_size // 2)\n",
        "        self.fc = nn.Linear(self.DistillBert.config.hidden_size, word_embedding_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        tokens = text[:, 0, :]\n",
        "        atts = text[:, 1, :]\n",
        "        text_vector = self.DistillBert(tokens, attention_mask=atts)[0]\n",
        "        text_vector = self.additive_attention(text_vector)\n",
        "        text_vector = self.fc(text_vector)\n",
        "        return text_vector"
      ],
      "metadata": {
        "id": "jWytyMB5cKej"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UserHistory Encoder\n",
        "This encodes the userHistory by using MultiHead Attention + Additive Attention Module"
      ],
      "metadata": {
        "id": "hJ_JkVUxjpBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
        "        scores = torch.exp(scores)\n",
        "        if attn_mask is not None:\n",
        "            scores = scores * attn_mask\n",
        "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_k, d_v):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model  # 300\n",
        "        self.n_heads = n_heads  # 20\n",
        "        self.d_k = d_k  # 20\n",
        "        self.d_v = d_v  # 20\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)  # 300, 400\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)  # 300, 400\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)  # 300, 400\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask=None):\n",
        "        batch_size, seq_len, _ = Q.size()\n",
        "\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.unsqueeze(1).expand(batch_size, seq_len, seq_len)\n",
        "            attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "\n",
        "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
        "        return context"
      ],
      "metadata": {
        "id": "NSQ9RHC_joey"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### News Updator Dataset"
      ],
      "metadata": {
        "id": "nrt_EPJHl72X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsUpdatorDataset(Dataset):\n",
        "    def __init__(self, news_index, news_ids, news_grads):\n",
        "        self.news_index = news_index\n",
        "        self.news_grads = news_grads\n",
        "        self.news_ids = news_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.news_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        nid = self.news_ids[idx]\n",
        "        return self.news_index[nid], self.news_grads[idx]"
      ],
      "metadata": {
        "id": "MQTtbNpSl-5E"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User Model\n",
        "This class contains the user model"
      ],
      "metadata": {
        "id": "LYxcSlqsVsBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 news_embedding_dim=400,\n",
        "                 num_attention_heads=20,\n",
        "                 query_vector_dim=200\n",
        "                 ):\n",
        "        super(UserEncoder, self).__init__()\n",
        "        self.dropout_rate = 0.2\n",
        "        self.multihead_attention = MultiHeadAttention(news_embedding_dim,\n",
        "                                                      num_attention_heads, 20, 20)\n",
        "        self.additive_attention = AdditiveAttention(news_embedding_dim,\n",
        "                                                    query_vector_dim)\n",
        "\n",
        "    def forward(self, clicked_news_vecs):\n",
        "        clicked_news_vecs = F.dropout(clicked_news_vecs, p=self.dropout_rate, training=self.training)\n",
        "        multi_clicked_vectors = self.multihead_attention(\n",
        "            clicked_news_vecs, clicked_news_vecs, clicked_news_vecs\n",
        "        )\n",
        "        pos_user_vector = self.additive_attention(multi_clicked_vectors)\n",
        "        user_vector = pos_user_vector\n",
        "        return user_vector\n"
      ],
      "metadata": {
        "id": "J401AUXZljF5"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UserModel(nn.Module):\n",
        "    def __init__(self, news_dataset, news_index, \n",
        "                 device, news_embedding_dim=400,\n",
        "                 num_attention_heads=20,\n",
        "                 query_vector_dim=200):\n",
        "        \n",
        "        super(UserModel, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.text_encoder = TextEncoder().to(device)\n",
        "        self.user_encoder = UserEncoder().to(device)\n",
        "\n",
        "        self.news_optimizer = optim.Adam(self.text_encoder.parameters(), lr=0.00005)\n",
        "        self.user_optimizer = optim.Adam(self.user_encoder.parameters(), lr=0.00005)\n",
        "\n",
        "        for param in self.text_encoder.DistillBert.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.news_dataset = news_dataset\n",
        "        self.news_index = news_index\n",
        "\n",
        "        self.time = 0\n",
        "        self.cnt = 0\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self._init_grad_param()\n",
        "\n",
        "    def _init_grad_param(self):\n",
        "        self.news_grads = {}\n",
        "        self.user_optimizer.zero_grad()\n",
        "        self.news_optimizer.zero_grad()\n",
        "\n",
        "    def gen_news_vecs(self, nids):\n",
        "        self.text_encoder.eval()\n",
        "        news_vectors = []\n",
        "        for i in nids:\n",
        "            news_ds = NewsPartDataset(self.news_index, i)\n",
        "            news_dl = DataLoader(news_ds, batch_size=2048, shuffle=False, num_workers=0)\n",
        "            \"\"\"\n",
        "            A dataloader that containers nid, news_index\n",
        "            \"\"\"\n",
        "            # news_vecs = np.zeros((npratio, 400), dtype='float32')\n",
        "            news_vecs = []\n",
        "            with torch.no_grad():\n",
        "                for nids, news in news_dl:\n",
        "                    news = news.to(self.device)\n",
        "                    news_vec = self.text_encoder(news).detach().cpu().numpy()\n",
        "                    # news_vecs[nids.numpy()] = news_vec\n",
        "                    news_vecs=news_vec\n",
        "            if np.isnan(news_vecs).any():\n",
        "                raise ValueError(\"news_vecs contains nan\")\n",
        "            news_vectors.append(news_vecs)\n",
        "        return news_vectors\n",
        "\n",
        "    def get_news_vecs(self, idx):\n",
        "        return self.news_vecs[idx]\n",
        "\n",
        "    def update(self):\n",
        "        self.update_user_grad()\n",
        "        self.update_news_grad()\n",
        "        self._init_grad_param()\n",
        "        self.cnt += 1\n",
        "\n",
        "    def average_update_time(self):\n",
        "        return self.time / self.cnt\n",
        "\n",
        "    def update_news_grad(self):\n",
        "        self.text_encoder.train()\n",
        "        self.news_optimizer.zero_grad()\n",
        "\n",
        "        news_ids, news_grads = [], []\n",
        "        for nid in self.news_grads:\n",
        "            news_ids.append(nid)\n",
        "            news_grads.append(self.news_grads[nid])\n",
        "\n",
        "        news_up_ds = NewsUpdatorDataset(self.news_index, news_ids, news_grads)\n",
        "        news_up_dl = DataLoader(news_up_ds, batch_size=128, shuffle=False, num_workers=0)\n",
        "        for news_index, news_grad in news_up_dl:\n",
        "            news_index = news_index.to(self.device)\n",
        "            news_grad = news_grad.to(self.device)\n",
        "            news_vecs = self.text_encoder(news_index)\n",
        "            news_vecs.backward(news_grad)\n",
        "\n",
        "        self.news_optimizer.step()\n",
        "        self.news_optimizer.zero_grad()\n",
        "\n",
        "    def update_user_grad(self):\n",
        "        self.user_optimizer.step()\n",
        "        self.user_optimizer.zero_grad()\n",
        "\n",
        "    def check_news_vec_same(self, nids, news_vecs):\n",
        "        assert (self.get_news_vecs(nids) == news_vecs).all(), \"News vecs are not the same\"\n",
        "\n",
        "    def collect(self, news_grad, user_grad):\n",
        "        # update user model params\n",
        "        for name, param in self.user_encoder.named_parameters():\n",
        "            if param.grad is None:\n",
        "                param.grad = user_grad[name]\n",
        "            else:\n",
        "                param.grad += user_grad[name]\n",
        "        # update news model params\n",
        "        for nid in news_grad:\n",
        "            if nid in self.news_grads:\n",
        "                self.news_grads[nid] += news_grad[nid]\n",
        "            else:\n",
        "                self.news_grads[nid] = news_grad[nid]\n",
        "\n",
        "    def forward(self, candidate_news, his_news, labels, compute_loss=True):\n",
        "        # print(\"Model is being called\")\n",
        "        candidate_vecs = torch.as_tensor(np.array(self.gen_news_vecs(candidate_news))).to(self.device)\n",
        "        # print(\"Successfully calculated the candidate vector\")\n",
        "\n",
        "        his_vec = torch.as_tensor(np.array(self.gen_news_vecs(his_news))).to(self.device)\n",
        "        # print(\"Successfully calculated the history vectors\")\n",
        "\n",
        "        candidate_vecs.requires_grad = True\n",
        "        his_vec.requires_grad = True\n",
        "\n",
        "        # print(\"Shape of candidate_news\", np.array(candidate_news).shape)\n",
        "        # print(\"Shape of history news\", np.array(his_news).shape)\n",
        "        # print(\"The shape of candidate_vecs\", candidate_vecs.shape)\n",
        "        # print(\"The shape of his vecs\", his_vec.shape)\n",
        "\n",
        "        user_vector = self.user_encoder(his_vec)    \n",
        "\n",
        "        score = torch.bmm(candidate_vecs, user_vector.unsqueeze(-1)).squeeze(dim=-1)\n",
        "       \n",
        "        score = torch.sigmoid(score)\n",
        "\n",
        "        if compute_loss:\n",
        "            loss = self.criterion(score, labels)\n",
        "            return loss, score, candidate_vecs, his_vec\n",
        "        else:\n",
        "            return score, candidate_vecs, his_vec"
      ],
      "metadata": {
        "id": "O1C416s0Vlqo"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Starts Here"
      ],
      "metadata": {
        "id": "gJhydBDlVygh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the client's data"
      ],
      "metadata": {
        "id": "dJwgsfTIoTL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading all the data for our LuckyUser\n",
        "with open(data_path / \"bert_nid2index.pkl\", \"rb\") as f:\n",
        "    nid2index = pickle.load(f)\n",
        "\n",
        "news_index = np.load(data_path / \"bert_news_index.npy\", allow_pickle=True)\n",
        "\n",
        "with open(data_path / \"train_sam_uid.pkl\", \"rb\") as f:\n",
        "    train_sam = pickle.load(f)\n",
        "\n",
        "with open(data_path / \"valid_sam_uid.pkl\", \"rb\") as f:\n",
        "    valid_sam = pickle.load(f)\n",
        "\n",
        "with open(data_path / \"test_sam_uid.pkl\", \"rb\") as f:\n",
        "    test_sam = pickle.load(f)"
      ],
      "metadata": {
        "id": "uAQ06ljBVWrR"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train DataLoader\n",
        "This class creates a training dataloader and returns a batch of `candidate_news, candidate_news_vecs, his, his_vecs, label` when called forward "
      ],
      "metadata": {
        "id": "Lz-G6P6U-tMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newsample(nnn, ratio):\n",
        "    if ratio > len(nnn):\n",
        "        return nnn + [\"<unk>\"] * (ratio - len(nnn))\n",
        "    else:\n",
        "        return random.sample(nnn, ratio)"
      ],
      "metadata": {
        "id": "9pJqxu9dBlEs"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, train_sam, nid2index, news_index):\n",
        "        # Here samples is same as train_sam_uid stuff \n",
        "        self.news_index = news_index\n",
        "        self.nid2index = nid2index\n",
        "        self.train_sam = train_sam\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_sam)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # pos, neg, his, neg_his\n",
        "        _, pos, neg, his, _ = self.train_sam[idx]\n",
        "        neg = newsample(neg, npratio)\n",
        "        candidate_news = np.array([self.nid2index[n] for n in [pos] + neg])\n",
        "        his = np.array([self.nid2index[n] for n in his] + [0] * (max_his_len - len(his)))\n",
        "        label = np.array(0)\n",
        "        return candidate_news, his, label"
      ],
      "metadata": {
        "id": "2x41w0YB-sw0"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation Script"
      ],
      "metadata": {
        "id": "fLscoV4EIvaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def dcg_score(y_true, y_score, k=10):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
        "    return np.sum(gains / discounts)\n",
        "\n",
        "\n",
        "def ndcg_score(y_true, y_score, k=10):\n",
        "    best = dcg_score(y_true, y_true, k)\n",
        "    actual = dcg_score(y_true, y_score, k)\n",
        "    return actual / best\n",
        "\n",
        "\n",
        "def mrr_score(y_true, y_score):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order)\n",
        "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
        "    return np.sum(rr_score) / np.sum(y_true)\n",
        "\n",
        "\n",
        "def compute_amn(y_true, y_score):\n",
        "    auc = roc_auc_score(y_true,y_score)\n",
        "    mrr = mrr_score(y_true,y_score)\n",
        "    ndcg5 = ndcg_score(y_true,y_score,5)\n",
        "    ndcg10 = ndcg_score(y_true,y_score,10)\n",
        "    return auc, mrr, ndcg5, ndcg10"
      ],
      "metadata": {
        "id": "eqIOLzEqIypA"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_split(news_vecs, user_vecs, samples, nid2index):\n",
        "    all_rslt = []\n",
        "    for i in tqdm(range(len(samples))):\n",
        "        _, poss, negs, _, _ = samples[i]\n",
        "        user_vec = user_vecs[i]\n",
        "        y_true = [1] * len(poss) + [0] * len(negs)\n",
        "        news_ids = [nid2index[i] for i in poss + negs]\n",
        "        news_vec = news_vecs[news_ids]\n",
        "        y_score = np.multiply(news_vec, user_vec)\n",
        "        y_score = np.sum(y_score, axis=1)\n",
        "        try:\n",
        "            all_rslt.append(compute_amn(y_true, y_score))\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    return np.array(all_rslt)"
      ],
      "metadata": {
        "id": "qNrGBTBpJaIA"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UserDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 args,\n",
        "                 samples,\n",
        "                 news_vecs,\n",
        "                 nid2index):\n",
        "        self.samples = samples\n",
        "        self.args = args\n",
        "        self.news_vecs = news_vecs\n",
        "        self.nid2index = nid2index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        _, poss, negs, his, _ = self.samples[idx]\n",
        "        his = [self.nid2index[n] for n in his] + [0] * (self.args.max_his_len - len(his))\n",
        "        his = self.news_vecs[his]\n",
        "        return his"
      ],
      "metadata": {
        "id": "3ppTkZteJixQ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_sam, nid2index, news_index, device):\n",
        "    # agg.gen_news_vecs(list(range(len(news_index))))\n",
        "    # model.user_encoder.eval()\n",
        "    model.eval()\n",
        "    news_vectors = model.gen_news_vecs()\n",
        "    user_dataset = UserDataset(valid_sam, agg.news_vecs, nid2index)\n",
        "    \"\"\"Returns history for a particular user\"\"\"\n",
        "    user_vecs = []\n",
        "    user_dl = DataLoader(user_dataset, batch_size=4096, shuffle=False, num_workers=0)\n",
        "    with torch.no_grad():\n",
        "        for his in tqdm(user_dl):\n",
        "            his = his.to(device)\n",
        "            user_vec = agg.user_encoder(his).detach().cpu().numpy()\n",
        "            user_vecs.append(user_vec)\n",
        "    user_vecs = np.concatenate(user_vecs)\n",
        "\n",
        "    val_scores = evaluation_split(agg.news_vecs, user_vecs, valid_sam, nid2index)\n",
        "    val_auc, val_mrr, val_ndcg, val_ndcg10 = [\n",
        "        np.mean(i) for i in list(zip(*val_scores))\n",
        "    ]\n",
        "\n",
        "    return val_auc, val_mrr, val_ndcg, val_ndcg10"
      ],
      "metadata": {
        "id": "ngVH5ZsPIxox"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training for each epoch"
      ],
      "metadata": {
        "id": "ttRf_RN3oXGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_train_steps = 500\n",
        "user_lr = 0.00005"
      ],
      "metadata": {
        "id": "SAs8IUTLb4RG"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_news_grad(candidate_info, his_info):\n",
        "    news_grad = {}\n",
        "    candidate_news, candidate_vecs, candidate_grad = candidate_info\n",
        "    his, his_vecs, his_grad = his_info\n",
        "\n",
        "    candidate_news, candaidate_grad = (\n",
        "        candidate_news.reshape(-1, ),\n",
        "        candidate_grad.reshape(-1, 400),\n",
        "    )\n",
        "    his, his_grad = his.reshape(-1, ), his_grad.reshape(-1, 400)\n",
        "\n",
        "    for nid, grad in zip(his, his_grad):\n",
        "        if nid in news_grad:\n",
        "            news_grad[nid] += grad\n",
        "        else:\n",
        "            news_grad[nid] = grad\n",
        "\n",
        "    for nid, grad in zip(candidate_news, candaidate_grad):\n",
        "        if nid in news_grad:\n",
        "            news_grad[nid] += grad\n",
        "        else:\n",
        "            news_grad[nid] = grad\n",
        "    return news_grad"
      ],
      "metadata": {
        "id": "qvEiV4ePxHn4"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_user_grad(model_param):\n",
        "    user_grad = {}\n",
        "    for name, param in model_param:\n",
        "        user_grad[name] = param.grad\n",
        "    return user_grad"
      ],
      "metadata": {
        "id": "AS8qIF8vxddk"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_on_step(model, user_indices, train_sam, nid2index, news_index, device):\n",
        "    # These are generating news vectors from transformers for given nids\n",
        "    train_ds = TrainDataset(train_sam, nid2index, news_index)\n",
        "    \"\"\" \n",
        "    Train_ds returns\n",
        "    candidate_news: Integer currsponding to news watched by one user sampled from set of users, \n",
        "    his: History of a user curresponding to a user, \n",
        "    label: 0\n",
        "    \"\"\"\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=10, shuffle=True, num_workers=0)\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    for cnt, batch_sample in enumerate(train_dl):\n",
        "        # model.user_encoder.load_state_dict(agg.user_encoder.state_dict())\n",
        "        optimizer = optim.SGD(model.parameters(), lr=user_lr)\n",
        "\n",
        "        candidate_news, his, label = batch_sample\n",
        "\n",
        "        bz_loss, y_hat, candidate_news_vecs, his_vecs = model(candidate_news, his, label)\n",
        "        loss += bz_loss.detach().cpu().numpy()\n",
        "\n",
        "        print(\"The loss we have encountered is\", loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        bz_loss.backward()\n",
        "\n",
        "        candaidate_grad = candidate_news_vecs.grad.detach().cpu() \n",
        "\n",
        "        candidate_vecs = candidate_news_vecs.detach().cpu().numpy()\n",
        "        candidate_news = candidate_news.numpy()\n",
        "\n",
        "        his_grad = his_vecs.grad.detach().cpu() \n",
        "        his_vecs = his_vecs.detach().cpu().numpy()\n",
        "        his = his.numpy()\n",
        "\n",
        "        news_grad = process_news_grad(\n",
        "            [candidate_news, candidate_vecs, candaidate_grad], [his, his_vecs, his_grad]\n",
        "        )\n",
        "        \"\"\"\n",
        "        Return a news grad: a dic that contains nid: gradient_curresponding to that news\n",
        "        \"\"\"\n",
        "\n",
        "        user_grad = process_user_grad(\n",
        "            model.user_encoder.named_parameters()\n",
        "        )\n",
        "        model.collect(news_grad, user_grad)\n",
        "\n",
        "    model.update()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "IHEfCwrAckFj"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset = NewsDataset(news_index)\n",
        "model = UserModel(news_dataset, news_index, device).to(device)"
      ],
      "metadata": {
        "id": "CIz3BqS2dAjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396f67a1-8e45-4035-e479-43f13f61680c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iteration = []\n",
        "loss_train = []\n",
        "for i in range(10):\n",
        "    loss = train_on_step(\n",
        "            model,\n",
        "            user_indices,\n",
        "            train_sam,\n",
        "            nid2index,\n",
        "            news_index,\n",
        "            device,\n",
        "        )\n",
        "    iteration.append(i)\n",
        "    loss_train.append(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIl0A-KJV_9o",
        "outputId": "7eb35d0b-be6c-45fd-8fcc-ea27066f1394"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss we have encountered is 1.6263298988342285\n",
            "The loss we have encountered is 1.5809887647628784\n",
            "The loss we have encountered is 1.5777842998504639\n",
            "The loss we have encountered is 1.5222123861312866\n",
            "The loss we have encountered is 1.473042368888855\n",
            "The loss we have encountered is 1.4577553272247314\n",
            "The loss we have encountered is 1.4685317277908325\n",
            "The loss we have encountered is 1.3857804536819458\n",
            "The loss we have encountered is 1.3778367042541504\n",
            "The loss we have encountered is 1.3351210355758667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(iteration, loss_train)\n",
        "plt.xlabel('Iteration Index')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ubMs3LkZTWyO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "7e0fbf02-bd88-406b-ee72-8a5bf982ef4c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJ9BJt9CTWvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUsrd_bFMxp_"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPODubw-Mxnn"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qpofmOW1MxlT"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Q89ogEdMxi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4lE2DAdtMxgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9AeTO1_Mxd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WVN6pC3MMxbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9b3PrjU8MxZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5L1XTXZKMxWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_auc = 0\n",
        "for step in range(max_train_steps):\n",
        "    loss = train_on_step(\n",
        "        model,\n",
        "        user_indices,\n",
        "        train_sam,\n",
        "        nid2index,\n",
        "        news_index,\n",
        "        device,\n",
        "    )\n",
        "\n",
        "    # wandb.log({\"train loss\": loss}, step=step + 1)\n",
        "\n",
        "    if (step + 1) % validation_steps == 0:\n",
        "        val_auc, val_mrr, val_ndcg, val_ndcg10 = validate(\n",
        "        agg, valid_sam, nid2index, news_index, device\n",
        "        )\n",
        "\n",
        "        # wandb.log(\n",
        "        #     {\n",
        "        #         \"valid auc\": val_auc,\n",
        "        #         \"valid mrr\": val_mrr,\n",
        "        #         \"valid ndcg@5\": val_ndcg,\n",
        "        #         \"valid ndcg@10\": val_ndcg10,\n",
        "        #     },\n",
        "        #   \n",
        "\n",
        "        with open(out_path / f\"log.txt\", \"a\") as f:\n",
        "            f.write(\n",
        "                f\"[{step}] round auc: {val_auc:.4f}, mrr: {val_mrr:.4f}, ndcg5: {val_ndcg:.4f}, ndcg10: {val_ndcg10:.4f}\\n\"\n",
        "            )\n",
        "\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            wandb.run.summary[\"best_auc\"] = best_auc\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"text_encoder\": agg.text_encoder.state_dict(),\n",
        "                    \"user_encoder\": agg.user_encoder.state_dict(),\n",
        "                },\n",
        "                out_model_path / f\"{args.name}-{args.data}.pkl\",\n",
        "            )\n",
        "\n",
        "            with open(out_path / f\"log.txt\", \"a\") as f:\n",
        "                f.write(f\"[{step}] round save model\\n\")"
      ],
      "metadata": {
        "id": "59HzonncTWtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0s3rtzzbTWqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}